### Ticket: TASK-03: Develop a Simple 3D CNN with Aggressive Regularization

#### 1. Description
This ticket covers the implementation of an advanced, experimental model using a 3D Convolutional Neural Network (CNN). The goal is to create a model that can learn relevant spatial features directly from the raw voxel data, providing an end-to-end learning approach.

The primary challenge and focus of this implementation is to combat the extreme risk of overfitting, given our small dataset size (~4k samples) and the massive input dimensionality (~900k voxels). Our strategy is not to build a complex, deep architecture, but rather the opposite: to design a **very shallow CNN and apply aggressive, multi-layered regularization** at every possible stage. This will force the model to learn only the most robust and generalizable features from the binary lesion maps.

The implementation will include:
1.  A PyTorch `Dataset` class for efficiently loading lesion data on-the-fly.
2.  A simple 3D CNN architecture built with standard PyTorch modules, featuring heavy use of `BatchNorm`, `Dropout`, and aggressive pooling.
3.  A training script that handles both regression (Task 1) and classification (Task 2), incorporating strong L2 regularization (`AdamW`) and Early Stopping to prevent overfitting.

#### 2. Justification
This approach, while experimental, is grounded in our core principles and is a logical extension of our previous work:

*   **Principle of Least Surprise (POLS):** We will use standard `torch.nn` modules (`Conv3d`, `BatchNorm3d`, `MaxPool3d`). The architecture will be a simple, sequential stack of Conv-BN-ReLU-Pool blocks, which is the most common and well-understood pattern in deep learning. This makes the model easy to build, debug, and for any developer to understand, avoiding "clever" or obscure architectures.
*   **Strict Contracts:** The model will be implemented as a `torch.nn.Module` with a clearly defined `forward` method, establishing a strict computational contract. The data it consumes will come from a custom PyTorch `Dataset` that wraps the validated DataFrame from `CORE-01`, ensuring type safety and consistency.
*   **Fail Fast, Fail Loudly:** The training script will implement robust validation and **Early Stopping**. If the model's performance on a held-out validation set ceases to improve or starts to degrade, the training process will terminate automatically. This saves computational resources and ensures that the final saved model is the one that generalized best, preventing us from shipping an overfitted model.
*   **DRY (Don't Repeat Yourself):** This ticket directly consumes the `train.csv` and `test.csv` files generated by `CORE-01`. All data cleaning, validation, and splitting logic is inherited from that foundational ticket and is not duplicated here.

#### 3. Implementation Plan

**Step 1: Create the PyTorch `Dataset` Class**

1.  Create a new file: `src/lesion_analysis/models/torch_loader.py`.
2.  Implement a custom `Dataset` class. This class will load NIfTI files from disk individually as requested by the `DataLoader`, which is highly memory-efficient.

    ```python
    # File: src/lesion_analysis/models/torch_loader.py
    
    import torch
    import pandas as pd
    import numpy as np
    import nibabel as nib
    from torch.utils.data import Dataset
    from typing import Tuple
    
    class LesionDataset(Dataset):
        """
        PyTorch Dataset for loading lesion NIfTI files on-the-fly.
        """
        def __init__(self, df: pd.DataFrame, target_col: str):
            self.df = df.reset_index(drop=True)
            self.target_col = target_col
    
        def __len__(self) -> int:
            return len(self.df)
    
        def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:
            """
            Loads a single lesion map and its corresponding target.
            """
            record = self.df.iloc[idx]
            filepath = record["lesion_filepath"]
            
            # Load image, ensure it has a channel dimension, and convert to tensor
            img = nib.load(filepath)
            data = torch.from_numpy(img.get_fdata(dtype=np.float32)).unsqueeze(0) # Shape: (1, 91, 109, 91)
            
            target_val = record[self.target_col]
            if pd.isna(target_val):
                 # Handle potential NaNs in boolean column for samples not in treatment group
                 target_val = 0 
            target = torch.tensor(target_val, dtype=torch.float32)
            
            return data, target
    ```

**Step 2: Define the CNN Architecture**

1.  Create a new file: `src/lesion_analysis/models/cnn.py`.
2.  Define the shallow, heavily regularized 3D CNN. Note the low channel count, aggressive pooling, and high dropout rate.

    ```python
    # File: src/lesion_analysis/models/cnn.py
    
    import torch
    import torch.nn as nn
    
    class Simple3DCNN(nn.Module):
        """
        A simple, shallow 3D CNN designed to minimize overfitting.
        
        Architecture: 3 blocks of Conv-BatchNorm-ReLU-MaxPool-Dropout,
                      followed by a global average pooling and a single linear layer.
        """
        def __init__(self, in_channels: int = 1, num_classes: int = 1, dropout_rate: float = 0.5):
            super().__init__()
            self.net = nn.Sequential(
                # Block 1
                nn.Conv3d(in_channels, 8, kernel_size=3, padding=1),
                nn.BatchNorm3d(8),  # Normalizes activations, acts as regularizer
                nn.ReLU(),
                nn.MaxPool3d(2),    # Aggressively downsample (91,109,91) -> (45,54,45)
                nn.Dropout3d(dropout_rate), # High dropout rate
    
                # Block 2
                nn.Conv3d(8, 16, kernel_size=3, padding=1),
                nn.BatchNorm3d(16),
                nn.ReLU(),
                nn.MaxPool3d(2),    # -> (22, 27, 22)
                nn.Dropout3d(dropout_rate),
                
                # Block 3
                nn.Conv3d(16, 32, kernel_size=3, padding=1),
                nn.BatchNorm3d(32),
                nn.ReLU(),
                nn.MaxPool3d(2),    # -> (11, 13, 11)
                nn.Dropout3d(dropout_rate),
    
                nn.AdaptiveAvgPool3d(1), # Global Average Pooling to (32, 1, 1, 1)
                nn.Flatten(),
                nn.Linear(32, num_classes)
            )
    
        def forward(self, x: torch.Tensor) -> torch.Tensor:
            return self.net(x).squeeze(-1) # Squeeze the final dimension
    ```

**Step 3: Create the Training Script**

1.  Create `scripts/train_cnn_models.py`. This script will contain the full training and validation loop. *Note: A production-grade implementation would refactor this into smaller components, but for this self-contained ticket, a single script is acceptable.*
2.  **Key Logic for the Script:**
    *   Use `argparse` to allow selecting which task to run (e.g., `python ... --task task1`).
    *   Load the `train.csv` file from `data/processed`.
    *   Create an internal 80/20 train/validation split from the main training data to monitor for overfitting.
    *   Instantiate the `LesionDataset` and `DataLoader` for both train and validation sets.
    *   **Task 1 (Regression):**
        *   Filter data: `df[df.clinical_score > 0]`.
        *   Model: `Simple3DCNN(num_classes=1)`.
        *   Loss: `nn.MSELoss()`.
        *   Optimizer: `torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-2)`. The `weight_decay` is critical for L2 regularization.
        *   Early Stopping: Monitor validation RMSE. Save the model checkpoint whenever validation RMSE improves. Stop training if it fails to improve for `patience=5` epochs.
    *   **Task 2 (Classification):**
        *   Filter data: `df[df.treatment_assignment.notna()]`.
        *   Model: `Simple3DCNN(num_classes=1)`.
        *   Loss: `nn.BCEWithLogitsLoss(pos_weight=pos_weight)`. Calculate `pos_weight` to handle class imbalance: `pos_weight = num_negative_samples / num_positive_samples`.
        *   Optimizer: `AdamW` with `weight_decay`.
        *   Early Stopping: Monitor validation Balanced Accuracy. Save the model whenever validation B.Acc improves. Stop training if it fails to improve for `patience=5` epochs.
    *   The script should save the best model weights to `models/task1_cnn_model.pt` and `models/task2_cnn_model.pt`.

#### 4. Acceptance Criteria
*   [ ] The file `src/lesion_analysis/models/torch_loader.py` exists with the `LesionDataset` class.
*   [ ] The file `src/lesion_analysis/models/cnn.py` exists with the `Simple3DCNN` class.
*   [ ] The script `scripts/train_cnn_models.py` can be executed from the command line for both Task 1 and Task 2.
*   [ ] The script generates and saves two model files: `models/task1_cnn_model.pt` and `models/task2_cnn_model.pt`. These files should contain the `state_dict` of the best performing model based on the validation metric.
*   [ ] The training process logs key metrics (Loss, RMSE for Task 1; Loss, Balanced Accuracy for Task 2) for both training and validation sets at the end of each epoch.
*   [ ] The script includes a functional Early Stopping mechanism that terminates training if the validation metric does not improve.

#### 5. Testing Requirements
*   **Unit Tests:**
    *   Create `tests/models/test_cnn.py`.
        *   **Test Case: Model Shape Contract.** Instantiate `Simple3DCNN`. Pass a dummy `torch.Tensor` of the expected input shape (e.g., `(2, 1, 91, 109, 91)`) through the model's `forward` method. Assert that the output tensor has the correct shape `(2,)`.
    *   Create `tests/models/test_torch_loader.py`.
        *   **Test Case: Dataset Loading.** Write a test for `LesionDataset`. Use a fixture to create a mock DataFrame and a dummy NIfTI file. Get an item using `dataset[0]` and assert that the returned data tensor has the shape `(1, 91, 109, 91)` and the target tensor is a scalar.
*   **Integration Tests (Smoke Test):**
    *   Create `tests/scripts/test_train_cnn.py`.
    *   Add an `argparse` argument to `scripts/train_cnn_models.py` called `--smoke-test`.
    *   If `--smoke-test` is enabled, the script should be modified to run for only a **single training batch and a single validation batch** and then exit successfully.
    *   The integration test will execute the script with this flag for both tasks: `python scripts/train_cnn_models.py --task task1 --smoke-test`.
    *   This test does not check for model convergence. It verifies that the entire pipeline—data loading, model forward pass, loss calculation, backward pass, and optimizer step—is correctly wired and executes without crashing.

#### 6. Definition of Done
*   [ ] All Acceptance Criteria are met.
*   [ ] All required unit and smoke tests are written and pass.
*   [ ] The code has been formatted (`black`), linted (`ruff`), and type-checked (`mypy`) successfully.
*   [ ] The project `README.md` is updated to include instructions for setting up PyTorch and running the CNN training script.
*   [ ] The code has been peer-reviewed and approved.