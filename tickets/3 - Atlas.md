### Ticket: TASK-02: Implement Atlas-Based Models for Prediction, Prescription, and Inference

#### 1. Description
This ticket details the implementation of our primary modeling strategy, which leverages domain knowledge in the form of a standard brain atlas to perform intelligent feature engineering. Instead of using hundreds of thousands of individual voxels as features, we will transform each high-dimensional lesion map into a low-dimensional vector of ~400 features, where each feature represents the proportion of damage ("lesion load") within a specific, anatomically defined brain Region of Interest (ROI).

This approach dramatically reduces dimensionality, mitigates the risk of overfitting, and produces highly interpretable results. The models trained on these features will provide a strong performance baseline for all three tasks and will be our main candidate solution.

The implementation will cover:
1.  A visual verification of the dataset's anatomical registration.
2.  A reusable feature extraction class based on the Schaefer 2018 atlas.
3.  Training an `ElasticNetCV` model for deficit prediction (Task 1).
4.  Training an `XGBClassifier` for treatment responder prediction (Task 2).
5.  Generating interpretable anatomical inference maps directly from model parameters (Task 3).

#### 2. Justification
This approach is chosen for its optimal balance of performance, interpretability, and data efficiency, directly aligning with our core principles:

*   **Principle of Least Surprise (POLS):** This is a standard, validated technique in the neuroimaging community (Lesion-to-Symptom Mapping). The outputs are directly interpretable: the feature importances from our models correspond to well-defined anatomical brain regions. This provides a clear, defensible answer to the inference task (Task 3) without requiring complex post-hoc interpretation methods.
*   **DRY (Don't Repeat Yourself):** The `AtlasFeatureExtractor` class will be a single, reusable component for transforming lesion data into features. The training script will consume the `train.csv` and `test.csv` files generated by `CORE-01`, ensuring we build upon our established, validated data pipeline.
*   **Strict Contracts:** The feature extractor will have a well-defined interface, accepting a pandas DataFrame (with a contract enforced by `CORE-01`) and returning a NumPy array. The models used (`ElasticNetCV`, `XGBClassifier`) have well-understood APIs and behaviors.
*   **Fail Fast, Fail Loudly:** The initial registration verification step is critical. If we discover that the lesions are not properly aligned with the standard MNI anatomical space, this entire approach is invalid. Performing this check upfront prevents us from building an entire solution on a faulty premise.

#### 3. Implementation Plan

**Step 1: Anatomical Registration Verification (EDA Notebook)**

1.  Create a new Jupyter Notebook: `notebooks/01_data_validation.ipynb`.
2.  Add the following code to compute and visualize a mean lesion map overlaid on the MNI template.

    ```python
    # File: notebooks/01_data_validation.ipynb
    
    import pandas as pd
    import numpy as np
    import nibabel as nib
    from nilearn import image, plotting, datasets
    from pathlib import Path
    
    # Load a sample of the data
    train_df = pd.read_csv("../data/processed/train.csv")
    sample_df = train_df.sample(n=100, random_state=42)
    
    # Compute the mean of the lesion maps
    mean_lesion_img = image.mean_img(sample_df['lesion_filepath'].tolist())
    
    # Load the MNI template
    mni_template = datasets.load_mni152_template(resolution=2)
    
    # Plot the overlay
    print("Plotting mean lesion map on MNI template...")
    plotting.plot_stat_map(
        mean_lesion_img,
        bg_img=mni_template,
        title="Mean Lesion Map Overlay (100 Samples)",
        display_mode="z",
        cut_coords=8,
        threshold=0.01, # Threshold to show only regions with >1% lesion frequency
        colorbar=True,
    )
    plotting.show()
    ```
3.  **Action:** Run the notebook. Visually inspect the output plot. You should see a "cloud" representing the average lesion location that is clearly situated *within* the brain boundaries of the MNI template. If the cloud is shifted, nonsensical, or doesn't respect major anatomical structures (e.g., the skull), stop and report a critical data quality issue. We will proceed assuming this check passes.

**Step 2: Implement Atlas-Based Feature Extractor**

1.  Create a new file: `src/lesion_analysis/features/atlas.py`.
2.  Add the `AtlasFeatureExtractor` class. We use the Schaefer 2018 atlas as it is a modern, widely-used, and functionally-informed parcellation.

    ```python
    # File: src/lesion_analysis/features/atlas.py
    
    import joblib
    import numpy as np
    import pandas as pd
    from nilearn.datasets import fetch_schaefer_2018_2mm
    from nilearn.maskers import NiftiLabelsMasker
    from pathlib import Path
    
    class AtlasFeatureExtractor:
        """
        Transforms NIfTI lesion maps into feature vectors based on an anatomical atlas.
        Each feature represents the mean lesion signal (lesion load) within an ROI.
        """
        def __init__(self, n_rois: int = 400, model_dir: Path = Path("models")):
            self.n_rois = n_rois
            self.atlas_name = f"schaefer_2018_{n_rois}rois"
            self.model_dir = model_dir
            self.model_dir.mkdir(exist_ok=True)
            self.masker_path = self.model_dir / f"{self.atlas_name}_masker.joblib"
            self.masker: NiftiLabelsMasker = None
    
        def fit(self):
            """
            Fetches the specified atlas, creates a NiftiLabelsMasker, and saves it to disk.
            This should be run once before training.
            """
            if self.masker_path.exists():
                print(f"Masker already exists at {self.masker_path}. Loading it.")
                self.masker = joblib.load(self.masker_path)
                return
    
            print("Fetching atlas and creating masker...")
            # Using the 2mm resolution version to match the data
            atlas = fetch_schaefer_2018_2mm(n_rois=self.n_rois)
            
            self.masker = NiftiLabelsMasker(
                labels_img=atlas.maps,
                standardize=False,  # We want lesion load, not z-scored values
                memory="nilearn_cache",
                verbose=0,
            )
            joblib.dump(self.masker, self.masker_path)
            print(f"Masker saved to {self.masker_path}")
    
        def transform(self, df: pd.DataFrame) -> np.ndarray:
            """
            Transforms lesion files into ROI-based feature matrix.
            
            Args:
                df: DataFrame with a 'lesion_filepath' column.
            
            Returns:
                A numpy array of shape (n_samples, n_rois).
            """
            if self.masker is None:
                if not self.masker_path.exists():
                    raise FileNotFoundError("Masker not found. Please run .fit() first.")
                self.masker = joblib.load(self.masker_path)
            
            lesion_filepaths = df["lesion_filepath"].tolist()
            print(f"Transforming {len(lesion_filepaths)} lesions into {self.n_rois} ROI features...")
            return self.masker.transform(lesion_filepaths)
    ```

**Step 3: Implement the Atlas Model Training Script**

1.  Create a new file: `scripts/train_atlas_models.py`.
2.  Add the complete script to orchestrate feature extraction, model training, and inference map generation.

    ```python
    # File: scripts/train_atlas_models.py
    
    import joblib
    import numpy as np
    import pandas as pd
    from pathlib import Path
    import nilearn.image as nli
    from sklearn.linear_model import ElasticNetCV
    from sklearn.utils.class_weight import compute_sample_weight
    from xgboost import XGBClassifier
    
    from src.lesion_analysis.features.atlas import AtlasFeatureExtractor
    
    # --- Configuration ---
    PROJECT_ROOT = Path(__file__).resolve().parent.parent
    PROCESSED_DATA_DIR = PROJECT_ROOT / "data" / "processed"
    MODELS_DIR = PROJECT_ROOT / "models"
    RESULTS_DIR = PROJECT_ROOT / "results"
    
    MODELS_DIR.mkdir(exist_ok=True)
    RESULTS_DIR.mkdir(exist_ok=True)
    
    # --- 1. Load Data ---
    print("Loading data...")
    train_df = pd.read_csv(PROCESSED_DATA_DIR / "train.csv")
    
    # --- 2. Feature Engineering ---
    feature_extractor = AtlasFeatureExtractor(n_rois=400, model_dir=MODELS_DIR)
    feature_extractor.fit() # Creates and saves the masker if it doesn't exist
    
    X_train_all = feature_extractor.transform(train_df)
    
    # --- 3. Task 1: Deficit Prediction (Regression) ---
    print("\n--- Training Task 1 Atlas Model (ElasticNetCV) ---")
    task1_mask = (train_df.clinical_score > 0).values
    X_task1 = X_train_all[task1_mask]
    y_task1 = train_df.loc[task1_mask, 'clinical_score'].values
    
    model_task1 = ElasticNetCV(cv=5, random_state=42, l1_ratio=[.1, .5, .7, .9, .95, .99, 1])
    model_task1.fit(X_task1, y_task1)
    
    joblib.dump(model_task1, MODELS_DIR / "task1_atlas_model.pkl")
    print(f"Task 1 model saved. Best L1 ratio: {model_task1.l1_ratio_:.2f}, Alpha: {model_task1.alpha_:.4f}")
    
    # --- 4. Task 2: Responder Prediction (Classification) ---
    print("\n--- Training Task 2 Atlas Model (XGBoost) ---")
    task2_mask = train_df.treatment_assignment.notna().values
    X_task2 = X_train_all[task2_mask]
    y_task2 = train_df.loc[task2_mask, 'is_responder'].values
    
    sample_weights = compute_sample_weight(class_weight='balanced', y=y_task2)
    
    model_task2 = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)
    model_task2.fit(X_task2, y_task2, sample_weight=sample_weights)
    
    joblib.dump(model_task2, MODELS_DIR / "task2_atlas_model.pkl")
    print("Task 2 model saved.")
    
    # --- 5. Task 3: Inference Maps ---
    print("\n--- Generating Atlas-Based Inference Maps ---")
    
    # Deficit Map from Task 1 model coefficients
    deficit_weights = model_task1.coef_
    deficit_map_img = feature_extractor.masker.inverse_transform(deficit_weights)
    deficit_map_img.to_filename(RESULTS_DIR / "deficit_map_atlas.nii.gz")
    print("Deficit map saved.")
    
    # Treatment Map from Task 2 model feature importances
    treatment_weights = model_task2.feature_importances_
    treatment_map_img = feature_extractor.masker.inverse_transform(treatment_weights)
    
    # Enforce subset constraint: treatment map must be a subset of deficit map
    # Binarize the deficit map (only regions with non-zero coefficients matter)
    deficit_mask_img = nli.math_img("np.abs(img) > 1e-6", img=deficit_map_img)
    final_treatment_map_img = nli.math_img("img1 * img2", img1=treatment_map_img, img2=deficit_mask_img)
    final_treatment_map_img.to_filename(RESULTS_DIR / "treatment_map_atlas.nii.gz")
    print("Treatment map saved.")
    
    print("\nAtlas model training script finished.")
    ```

#### 4. Acceptance Criteria
*   [ ] The notebook `notebooks/01_data_validation.ipynb` successfully runs and produces a plausible overlay plot of the mean lesion map on the MNI template.
*   [ ] The file `src/lesion_analysis/features/atlas.py` exists and contains the `AtlasFeatureExtractor` class.
*   [ ] The script `scripts/train_atlas_models.py` runs to completion without errors.
*   [ ] The script generates and saves the following output files:
    *   `models/schaefer_2018_400rois_masker.joblib`
    *   `models/task1_atlas_model.pkl`
    *   `models/task2_atlas_model.pkl`
    *   `results/deficit_map_atlas.nii.gz`
    *   `results/treatment_map_atlas.nii.gz`
*   [ ] The generated `.nii.gz` maps are 3D volumes that can be opened in a standard NIfTI viewer.

#### 5. Testing Requirements
*   **Unit Tests:**
    *   Create `tests/features/test_atlas.py`.
    *   Write a test for `AtlasFeatureExtractor.transform`.
        *   Use a fixture to create a temporary directory with dummy 3D NIfTI files and a mock DataFrame.
        *   Mock the `nilearn.datasets.fetch_schaefer_2018_2mm` call to return a simple, predictable atlas image.
        *   Run `.fit()` and `.transform()` and assert the output feature matrix has the correct shape `(n_samples, n_rois)`.
*   **Integration Tests:**
    *   Create `tests/scripts/test_train_atlas_models.py`.
    *   Write a test that runs the `scripts/train_atlas_models.py` script as a subprocess or imported main function.
    *   Use a fixture to create a minimal `train.csv` file (e.g., 20 rows) pointing to real lesion files.
    *   Assert that all five output artifacts (`.joblib`, `.pkl`, `.nii.gz`) are created in their respective directories.
    *   Load the generated `.nii.gz` maps with `nibabel` and assert their dimensions are `(91, 109, 91)`.

#### 6. Definition of Done
*   [ ] All Acceptance Criteria are met.
*   [ ] All required unit and integration tests are written and pass.
*   [ ] The code has been formatted (`black`), linted (`ruff`), and type-checked (`mypy`) successfully.
*   [ ] The project `README.md` is updated to include instructions for running the atlas-based model training script.
*   [ ] The code has been peer-reviewed and approved.