### **Critical Issues & Required Refactors**

There are two critical flaws in the current implementation that violate our core doctrines. They require immediate attention and will be detailed in two new tickets: `CORE-02` and `REFACTOR-01`.

#### **1. The Pydantic Contract is Defined but Not Enforced**

*   **File:** `src/lesion_analysis/data/loader.py`
*   **Problem:** The `load_and_prepare_data` function performs Pydantic validation on the DataFrame records, which correctly converts `NaN` and `"N/A"` strings into `None`. However, this happens inside a list comprehension `_ = [LesionRecord(**row) for row in records]` where the result is discarded. The function then returns the *original, unmodified DataFrame*, with its `pd.NA` and `NaN` values still present.
*   **Consequence:** This is a severe violation of **Strict Contracts**. We have defined a clean contract (`LesionRecord`) but are not actually using the validated and cleaned data product. Downstream components believe they are receiving data that adheres to the contract, but they are not. This leads directly to the next critical bug.

#### **2. The PyTorch Dataset is Brittle and Will Crash on Real Data**

*   **File:** `src/lesion_analysis/models/torch_loader.py`
*   **Problem:** The `LesionDataset.__getitem__` method contains the line `target = torch.tensor(target_val, dtype=torch.float32)`. Because of the bug above, `target_val` for `is_responder` can be `pd.NA` for patients without a treatment assignment. `torch.tensor(pd.NA)` raises a `TypeError`. This will crash the CNN training script for Task 2 the moment the `DataLoader` encounters a patient without a treatment assignment. The corresponding test (`tests/models/test_torch_loader.py::test_dataset_handles_nan_targets`) incorrectly validates this broken behavior by asserting that an error is raised.
*   **Consequence:** This is a failure of **Fail Fast, Fail Loudly** and **Strict Contracts**. The `Dataset` has a brittle, implicit contract that it must receive clean data, but the upstream loader fails to provide it. The error doesn't happen at the data loading stage; it happens deep inside the training loop, making it harder to debug.

---

I will now create the tickets to fix these issues.

### Ticket: CORE-02: Fortify Data Pipeline and Enforce Strict Contracts

#### 1. Description
The current data pipeline has a critical bug where the Pydantic validation and cleaning step is performed, but its results are discarded. The data loader returns the original, un-validated DataFrame. This leads to downstream failures, particularly in the PyTorch `Dataset`, which cannot handle the `pd.NA` values that should have been cleaned. This ticket corrects the loader to return the fully validated and sanitized data and makes the PyTorch `Dataset` robust to any potential missing values.

#### 2. Justification
*   **Strict Contracts:** This fix is essential to enforce our data contracts. The `load_and_prepare_data` function will now guarantee that its output DataFrame has been successfully validated against the `LesionRecord` model. The PyTorch `Dataset` will be modified to have a more robust internal contract, correctly handling missing target values that are expected for certain data subsets.
*   **Fail Fast, Fail Loudly:** By returning a DataFrame built from validated Pydantic models, we ensure that any data type or integrity issues are caught and resolved at the earliest possible stage (`load_and_prepare_data`), not deep inside a model training loop.

#### 3. Implementation Plan
**Step 1: Fix the Data Loader (`src/lesion_analysis/data/loader.py`)**

1.  Modify `load_and_prepare_data` to reconstruct the DataFrame from the validated Pydantic records. This ensures the cleaned data (e.g., `None` instead of `NaN`) is actually used.

    ```python
    # In src/lesion_analysis/data/loader.py

    def load_and_prepare_data(csv_path: Path, lesions_dir: Path) -> pd.DataFrame:
        # ... (initial checks and path enrichment are unchanged) ...
        df = pd.read_csv(csv_path)
        df["lesion_filepath"] = df["lesion_id"].apply(lambda x: lesions_dir / x)

        # --- VALIDATION ---
        records = df.to_dict(orient="records")
        validated_records = [LesionRecord(**row) for row in records]
        
        # --- RECONSTRUCT DATAFRAME FROM VALIDATED DATA ---
        # This is the critical fix. We now use the clean data.
        df = pd.DataFrame([rec.model_dump() for rec in validated_records])

        # --- FEATURE ENGINEERING ---
        df["is_responder"] = (df["outcome_score"] < df["clinical_score"]).astype("boolean")
        df['treatment_assignment'] = df['treatment_assignment'].replace('N/A', pd.NA)

        return df
    ```

**Step 2: Make the PyTorch Dataset Robust (`src/lesion_analysis/models/torch_loader.py`)**

1.  Modify `LesionDataset.__getitem__` to safely handle potentially missing target values, which is expected behavior for certain subsets (e.g., patients without treatment have no `is_responder` label).

    ```python
    # In src/lesion_analysis/models/torch_loader.py

    # ... inside LesionDataset.__getitem__ ...
    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:
        record = self.df.iloc[idx]
        filepath = record["lesion_filepath"]

        img = nib.load(filepath)
        data = torch.from_numpy(img.get_fdata(dtype=np.float32)).unsqueeze(0)

        target_val = record[self.target_col]
        # Robustly handle missing values, default to 0.
        # This is safe because these samples will be filtered out by the training script anyway.
        if pd.isna(target_val):
            target_val = 0.0 
        
        target = torch.tensor(target_val, dtype=torch.float32)

        return data, target
    ```

**Step 3: Fix the Unit Test (`tests/models/test_torch_loader.py`)**

1.  The test `test_dataset_handles_nan_targets` is fundamentally incorrect. It should test that NaNs are handled gracefully, not that they cause a crash. Replace it.

    ```python
    # In tests/models/test_torch_loader.py

    def test_dataset_handles_nan_targets_gracefully(self, mock_nifti_file):
        """Test that dataset handles NaN targets by converting them to a default value."""
        df_with_nan = pd.DataFrame({
            "lesion_id": ["lesion001.nii.gz"],
            "lesion_filepath": [mock_nifti_file],
            "is_responder": [np.nan],
        })

        dataset = LesionDataset(df_with_nan, "is_responder")
        data, target = dataset[0] # Should NOT raise an error
        
        # Assert that the NaN was converted to a default value (e.g., 0.0)
        assert target.item() == 0.0
    ```

#### 4. Acceptance Criteria
*   [ ] The `load_and_prepare_data` function in `loader.py` is updated to return a DataFrame constructed from validated Pydantic models.
*   [ ] The `LesionDataset` in `torch_loader.py` is updated to gracefully handle `pd.NA` or `np.nan` in the target column.
*   [ ] The unit test `tests/models/test_torch_loader.py` is updated to correctly test for graceful handling of `NaN`s, not for errors.
*   [ ] The CNN training script (`scripts/train_cnn_models.py`) for Task 2 runs without crashing.

#### 5. Testing Requirements
*   **Unit Tests:**
    *   The existing tests for `loader.py` should be re-run and pass.
    *   The modified test in `test_torch_loader.py` must pass.
*   **Integration Tests:**
    *   The smoke test for `scripts/train_cnn_models.py --task task2` must pass, as it will now process the full (unfiltered) training set through the `DataLoader` before the script-level filtering occurs.

#### 6. Definition of Done
*   [ ] All Acceptance Criteria are met.
*   [ ] All required unit and integration tests are written/updated and pass.
*   [ ] The code has been formatted (`black`), linted (`ruff`), and type-checked (`mypy`) successfully.
*   [ ] The code has been peer-reviewed and approved.

---

### Ticket: REFACTOR-01: Standardize CNN Training and Evaluation Loop

#### 1. Description
The current CNN training script (`scripts/train_cnn_models.py`) contains a significant architectural inconsistency. While `scripts/prepare_data.py` creates a canonical train/test split for the project, the CNN script ignores this and creates its own internal 80/20 train/validation split from the main training set. This makes its validation results incomparable to any evaluation performed on the canonical test set and violates our project's data flow principles.

This ticket refactors the CNN training script to use a standardized train/validation/test workflow, making its results robust and comparable to other models.

#### 2. Justification
*   **DRY (Don't Repeat Yourself):** There must be a single source of truth for our data splits. This refactor ensures the CNN models are trained and evaluated using the exact same data partitions as the baseline and atlas models, making performance comparisons valid.
*   **Principle of Least Surprise (POLS):** A developer would expect a script in a project with a canonical train/test split to use that split. The current implementation is surprising and misleading. This change aligns the script's behavior with standard MLOps practice.

#### 3. Implementation Plan
**Step 1: Modify `scripts/prepare_data.py` to create a validation set**

1.  Update the data splitting script to produce three files: `train.csv`, `validation.csv`, and `test.csv`. A 70/15/15 split is a robust choice.

    ```python
    # In scripts/prepare_data.py

    # ... inside main() ...
    # Perform a 70/30 split first
    train_val_df, test_df = train_test_split(
        df, test_size=0.15, random_state=42, stratify=stratify_col
    )
    
    # Split the 70% into train and validation
    # New stratify column for the smaller set
    stratify_col_train_val = train_val_df['treatment_assignment'].fillna('None')
    train_df, val_df = train_test_split(
        train_val_df, test_size=0.1765, # 0.15 / 0.85 to get 15% of original
        random_state=42, stratify=stratify_col_train_val
    )
    
    # Save splits
    train_df.to_csv(PROCESSED_DATA_DIR / "train.csv", index=False)
    val_df.to_csv(PROCESSED_DATA_DIR / "validation.csv", index=False)
    test_df.to_csv(PROCESSED_DATA_DIR / "test.csv", index=False)
    ```

**Step 2: Refactor `scripts/train_cnn_models.py`**

1.  Remove the internal `train_test_split` from `setup_data_loaders`.
2.  The script should now load `train.csv` and `validation.csv` separately.

    ```python
    # In scripts/train_cnn_models.py

    def setup_data_loaders(...):
        train_full_df = pd.read_csv("data/processed/train.csv")
        val_full_df = pd.read_csv("data/processed/validation.csv")

        # ... (filter train_full_df and val_full_df based on task) ...
        
        train_dataset = LesionDataset(train_df_filtered, target_col)
        val_dataset = LesionDataset(val_df_filtered, target_col)
        
        # ... (create and return DataLoaders) ...
    ```

#### 4. Acceptance Criteria
*   [ ] `scripts/prepare_data.py` now generates three files: `train.csv`, `validation.csv`, and `test.csv`.
*   [ ] `scripts/train_cnn_models.py` no longer contains a call to `train_test_split`.
*   [ ] The `setup_data_loaders` function loads data from the canonical `train.csv` and `validation.csv` files.
*   [ ] The CNN training script runs successfully with the new data loading logic.

#### 5. Testing Requirements
*   **Integration Tests:** The smoke test for `scripts/train_cnn_models.py` must be updated to ensure the presence of both `train.csv` and `validation.csv` and must pass.

#### 6. Definition of Done
*   [ ] All Acceptance Criteria are met.
*   [ ] All required integration tests are updated and pass.
*   [ ] Code is formatted, linted, and type-checked.
*   [ ] The code has been peer-reviewed and approved.

---

### **Minor Code Smells and Recommendations**

*   **Weak `mypy` Configuration:** The `pyproject.toml` configuration for `mypy` is too permissive, ignoring all third-party libraries and disabling strict optional checks. This significantly reduces the value of static typing. **Recommendation:** Incrementally enable stricter checks (`--strict-optional`) and add type stubs (`types-*`) for major libraries like `pandas` and `numpy` to get meaningful type safety.
*   **Superfluous `masker.fit()`:** In `src/lesion_analysis/features/atlas.py`, the `self.masker.fit()` call is unnecessary. A `NiftiLabelsMasker` is configured by its `labels_img` at initialization and does not need to be fit to data before transforming. **Recommendation:** Remove the `self.masker.fit()` line to reduce confusion.
*   **Redundant `type: ignore`:** The training scripts contain `# type: ignore` comments for numpy boolean indexing. A better `mypy` configuration with numpy stubs would resolve these correctly.