### Ticket: MLOPS-01 (Revised): Create a Resilient, Standardized Evaluation Pipeline

#### 1. Description
The project currently lacks a standardized and robust method for comparing the final performance of all implemented models. A simple script that assumes all model files exist is brittle and will fail if any part of the training pipeline is incomplete.

This ticket introduces a single, authoritative, and **resilient** evaluation script: `scripts/evaluate_models.py`. This script will be responsible for:
1.  Loading the canonical `test.csv` dataset.
2.  Systematically checking for the existence of all expected final model artifacts (baseline, atlas, CNN).
3.  For each model that is found, loading it and its required features.
4.  Generating predictions on the test set.
5.  Computing and reporting the official evaluation metrics (RMSE for Task 1, Balanced Accuracy for Task 2).
6.  Gracefully skipping any model whose artifact is missing and reporting this clearly in the final output.
7.  Aggregating all successful evaluation results into a single, clean report in both the console and a CSV file.

This provides a single, reliable source of truth for model performance that is robust to partial training failures.

#### 2. Justification
This refactor is essential for building a reliable MLOps pipeline and is grounded in our core principles:

*   **Fail Fast, Fail Loudly (applied correctly):** The script should fail loudly *if the evaluation logic itself is flawed*. It should **not** fail if an optional, upstream dependency (a trained model file) is missing. By gracefully handling missing models, it provides maximum insight into the state of the project, reporting on what *did* succeed, which is more valuable than a wholesale crash.
*   **DRY (Don't Repeat Yourself):** All evaluation logic is centralized. Helper functions for loading data, generating predictions, and calculating metrics will be defined once and reused for each model type, making the script clean and maintainable.
*   **Strict Contracts:** The script establishes a firm contract for evaluation. It ingests standardized model artifacts and the canonical `test.csv`, and it outputs a standardized performance report. Its resilience adds a meta-contract: the script guarantees it will always produce a report, even if some inputs are missing.
*   **Principle of Least Surprise (POLS):** A user running an evaluation script expects a summary of what's available, not a crash because one component is missing. This behavior is predictable and provides more useful feedback to the developer.

#### 3. Implementation Plan

**Step 1: Design a Modular Structure in the Evaluation Script**

1.  Create the file `scripts/evaluate_models.py`.
2.  Instead of a linear script, structure it with a main function and helper functions for each model type. This allows for clean `try...except FileNotFoundError` blocks.

**Step 2: Implement the Full, Resilient Script**

This is the complete, implementation-ready code for `scripts/evaluate_models.py`.

```python
# File: scripts/evaluate_models.py

import pandas as pd
import numpy as np
import joblib
import torch
from pathlib import Path
import sys
from sklearn.metrics import mean_squared_error, balanced_accuracy_score
import logging

# --- Setup ---
# Add src to path for imports
sys.path.append(str(Path(__file__).parent.parent / "src"))

from src.lesion_analysis.features.voxel import downsample_and_flatten_lesions
from src.lesion_analysis.features.atlas import AtlasFeatureExtractor
from src.lesion_analysis.models.cnn import Simple3DCNN
from src.lesion_analysis.models.torch_loader import LesionDataset
from torch.utils.data import DataLoader

# Configure logging
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)

# --- Configuration ---
PROJECT_ROOT = Path(__file__).resolve().parent.parent
PROCESSED_DATA_DIR = PROJECT_ROOT / "data" / "processed"
MODELS_DIR = PROJECT_ROOT / "models"
RESULTS_DIR = PROJECT_ROOT / "results"
FEATURES_CACHE_DIR = PROCESSED_DATA_DIR / "features_cache"

RESULTS_DIR.mkdir(exist_ok=True)

# --- Helper Functions for each model type ---

def evaluate_baseline_models(test_df: pd.DataFrame) -> dict:
    """Evaluates the baseline models. Returns a dictionary of results."""
    logger.info("--- Evaluating Baseline Models ---")
    results = {}
    try:
        # Task 1 (Regression)
        model_path = MODELS_DIR / "task1_baseline_model.pkl"
        if not model_path.exists():
            raise FileNotFoundError(f"Model not found: {model_path}")
        
        model_b1 = joblib.load(model_path)
        df_t1 = test_df[test_df.clinical_score > 0]
        X_t1 = downsample_and_flatten_lesions(df_t1, FEATURES_CACHE_DIR)
        preds = model_b1.predict(X_t1)
        rmse = np.sqrt(mean_squared_error(df_t1.clinical_score, preds))
        results["Baseline_Task1_RMSE"] = rmse
        logger.info(f"  - Task 1 RMSE: {rmse:.4f}")

    except FileNotFoundError as e:
        logger.warning(f"  - Could not evaluate Baseline Task 1 model: {e}")

    try:
        # Task 2 (Classification)
        model_path = MODELS_DIR / "task2_baseline_model.pkl"
        if not model_path.exists():
            raise FileNotFoundError(f"Model not found: {model_path}")
        
        model_b2 = joblib.load(model_path)
        df_t2 = test_df[test_df.treatment_assignment.notna()]
        X_t2 = downsample_and_flatten_lesions(df_t2, FEATURES_CACHE_DIR)
        preds = model_b2.predict(X_t2)
        bacc = balanced_accuracy_score(df_t2.is_responder.astype(bool), preds)
        results["Baseline_Task2_BACC"] = bacc
        logger.info(f"  - Task 2 Balanced Accuracy: {bacc:.4f}")

    except FileNotFoundError as e:
        logger.warning(f"  - Could not evaluate Baseline Task 2 model: {e}")
        
    return results


def evaluate_atlas_models(test_df: pd.DataFrame) -> dict:
    """Evaluates the atlas-based models. Returns a dictionary of results."""
    logger.info("--- Evaluating Atlas Models ---")
    results = {}
    try:
        feature_extractor_atlas = AtlasFeatureExtractor(n_rois=400, model_dir=MODELS_DIR)
        
        # Task 1 (Regression)
        model_path = MODELS_DIR / "task1_atlas_model.pkl"
        if not model_path.exists():
            raise FileNotFoundError(f"Model not found: {model_path}")
            
        model_a1 = joblib.load(model_path)
        df_t1 = test_df[test_df.clinical_score > 0]
        X_t1_atlas = feature_extractor_atlas.transform(df_t1)
        preds = model_a1.predict(X_t1_atlas)
        rmse = np.sqrt(mean_squared_error(df_t1.clinical_score, preds))
        results["Atlas_Task1_RMSE"] = rmse
        logger.info(f"  - Task 1 RMSE: {rmse:.4f}")

    except FileNotFoundError as e:
        logger.warning(f"  - Could not evaluate Atlas Task 1 model: {e}")

    try:
        feature_extractor_atlas = AtlasFeatureExtractor(n_rois=400, model_dir=MODELS_DIR)

        # Task 2 (Classification)
        model_path = MODELS_DIR / "task2_atlas_model.pkl"
        if not model_path.exists():
            raise FileNotFoundError(f"Model not found: {model_path}")

        model_a2 = joblib.load(model_path)
        df_t2 = test_df[test_df.treatment_assignment.notna()]
        X_t2_atlas = feature_extractor_atlas.transform(df_t2)
        preds = model_a2.predict(X_t2_atlas)
        bacc = balanced_accuracy_score(df_t2.is_responder.astype(bool), preds)
        results["Atlas_Task2_BACC"] = bacc
        logger.info(f"  - Task 2 Balanced Accuracy: {bacc:.4f}")

    except FileNotFoundError as e:
        logger.warning(f"  - Could not evaluate Atlas Task 2 model: {e}")
        
    return results


def evaluate_cnn_models(test_df: pd.DataFrame) -> dict:
    """Evaluates the CNN models. Returns a dictionary of results."""
    logger.info("--- Evaluating CNN Models ---")
    results = {}
    device = "cuda" if torch.cuda.is_available() else "cpu"

    try:
        # Task 1 (Regression)
        model_path = MODELS_DIR / "task1_cnn_model.pt"
        if not model_path.exists():
            raise FileNotFoundError(f"Model not found: {model_path}")

        model_c1 = Simple3DCNN().to(device)
        model_c1.load_state_dict(torch.load(model_path, map_location=device))
        model_c1.eval()

        df_t1 = test_df[test_df.clinical_score > 0]
        ds_c1 = LesionDataset(df_t1, "clinical_score")
        loader_c1 = DataLoader(ds_c1, batch_size=8, shuffle=False)

        all_preds, all_targets = [], []
        with torch.no_grad():
            for imgs, targets in loader_c1:
                imgs = imgs.to(device)
                preds = model_c1(imgs)
                all_preds.extend(preds.cpu().numpy())
                all_targets.extend(targets.numpy())

        rmse = np.sqrt(mean_squared_error(all_targets, all_preds))
        results["CNN_Task1_RMSE"] = rmse
        logger.info(f"  - Task 1 RMSE: {rmse:.4f}")

    except FileNotFoundError as e:
        logger.warning(f"  - Could not evaluate CNN Task 1 model: {e}")
        
    try:
        # Task 2 (Classification)
        model_path = MODELS_DIR / "task2_cnn_model.pt"
        if not model_path.exists():
            raise FileNotFoundError(f"Model not found: {model_path}")

        model_c2 = Simple3DCNN().to(device)
        model_c2.load_state_dict(torch.load(model_path, map_location=device))
        model_c2.eval()

        df_t2 = test_df[test_df.treatment_assignment.notna()]
        ds_c2 = LesionDataset(df_t2, "is_responder")
        loader_c2 = DataLoader(ds_c2, batch_size=8, shuffle=False)

        all_preds, all_targets = [], []
        with torch.no_grad():
            for imgs, targets in loader_c2:
                imgs = imgs.to(device)
                logits = model_c2(imgs)
                preds = (torch.sigmoid(logits) > 0.5).cpu().numpy()
                all_preds.extend(preds)
                all_targets.extend(targets.numpy())
        
        bacc = balanced_accuracy_score(all_targets, all_preds)
        results["CNN_Task2_BACC"] = bacc
        logger.info(f"  - Task 2 Balanced Accuracy: {bacc:.4f}")

    except FileNotFoundError as e:
        logger.warning(f"  - Could not evaluate CNN Task 2 model: {e}")

    return results


def main():
    """Main function to run the evaluation pipeline."""
    try:
        test_df = pd.read_csv(PROCESSED_DATA_DIR / "test.csv")
    except FileNotFoundError:
        logger.error(f"FATAL: test.csv not found at {PROCESSED_DATA_DIR}. Please run scripts/prepare_data.py first.")
        return

    all_results = {}
    all_results.update(evaluate_baseline_models(test_df))
    all_results.update(evaluate_atlas_models(test_df))
    all_results.update(evaluate_cnn_models(test_df))

    if not all_results:
        logger.error("No models were found to evaluate. Please run training scripts.")
        return

    # --- Report Results ---
    logger.info("\n--- Final Model Performance on Test Set ---")
    results_df = pd.DataFrame.from_dict(all_results, orient="index", columns=["Score"])
    results_df.index.name = "Model_Metric"
    print(results_df.to_string())

    output_path = RESULTS_DIR / "evaluation_report.csv"
    results_df.to_csv(output_path)
    logger.info(f"\nFull report saved to {output_path}")

if __name__ == "__main__":
    main()
```

#### 4. Acceptance Criteria
*   [ ] The script `scripts/evaluate_models.py` is created and matches the implementation above.
*   [ ] The script runs to completion without crashing, even if one or more model artifact files are missing from the `models/` directory.
*   [ ] If a model artifact is missing, the script prints a clear `WARNING` message to the console and continues execution.
*   [ ] The script prints a final, formatted pandas DataFrame to the console summarizing the performance of all successfully evaluated models.
*   [ ] The script saves the performance summary to `results/evaluation_report.csv`.
*   [ ] If no models are found, the script prints an `ERROR` message and exits gracefully without creating a report file.

#### 5. Testing Requirements
*   **Integration Tests:**
    *   Create `tests/scripts/test_evaluate_models.py`.
    *   **Test Case 1 (All Models Present):** Write a test that first runs all three training scripts in `--smoke-test` mode to generate dummy model artifacts. Then, execute `scripts/evaluate_models.py`. Assert that the script runs successfully and that the output `evaluation_report.csv` contains entries for all model types.
    *   **Test Case 2 (Some Models Missing):** Write a test that generates artifacts for only the `baseline` and `atlas` models. Run `scripts/evaluate_models.py`. Assert that the script runs successfully, prints warnings about the missing CNN models, and the final report contains only baseline and atlas results.
    *   **Test Case 3 (No Models Present):** Write a test that ensures the `models/` directory is empty. Run `scripts/evaluate_models.py`. Assert that the script exits gracefully and prints an error message about no models being found.

#### 6. Definition of Done
*   [ ] All Acceptance Criteria are met.
*   [ ] The required integration tests are written and pass.
*   [ ] The code has been formatted (`black`), linted (`ruff`), and type-checked (`mypy`) successfully.
*   [ ] The `README.md` is updated to include instructions for running the resilient evaluation script.
*   [ ] The code has been peer-reviewed and approved.